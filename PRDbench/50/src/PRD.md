# PRD: NetEase Cloud Music Data Collection and Analysis Platform

## Requirements Overview

This platform is designed to be an intelligent data collection and analysis system for NetEase Cloud Music, built upon distributed crawler technology. The system utilizes the Scrapy framework to automate the collection of playlist data. It integrates data cleaning, storage, visual analysis, and machine learning to provide data-driven decision support for the music industry.

The platform's core value lies in its integration of five dimensions of visual data analysis, including song popularity distribution and user preference analysis. It also features a built-in machine learning model for predicting play counts, offering intelligent recommendations for content creators and platform operators.

The system targets music data analysts, content operations staff, and academic research institutions. By providing standardized data interfaces and visual reports, it lowers the technical barrier for music data analysis and enhances the efficiency and accuracy of data-driven insights. The system employs a command-line interface suitable for local development environments, with all visual results saved as local image files.

## Functional Requirements

### 2.1 Distributed Data Collection Module

#### 2.1.0 Data Source Configuration
- **Base URL**: `https://music.163.com`
- **Playlist Index Page URL Format**: `https://music.163.com/discover/playlist/?cat={category}&order=hot&limit=35&offset={offset}`
  - Parameter Description: `cat` (playlist category), `order=hot` (sorting method), `limit=35` (items per page), `offset={offset}` (pagination offset)
- **Playlist Detail Page URL Format**: `https://music.163.com{relative_path}` (relative path is parsed from the index page)

#### 2.1.1 Playlist Index Crawler Subsystem
-   **URL Queue Management**: Supports the dynamic generation of Chinese playlist URLs (with the `offset` parameter ranging from 0 to 1715, in steps of 35) to enable an automated URL push mechanism.
-   **Data Parsing Engine**: Uses the BeautifulSoup parser to extract key information from playlist index pages:
    -   Playlist Detail Page URL (retrieved via the CSS selector `.dec a` for the `href` attribute).
    -   Playlist Title (automatically converts English commas to Chinese commas to prevent CSV format conflicts).
    -   Play Count Data (supports automatic conversion of the Chinese character for "ten thousand" unit to a numeric value).
    -   Creator's Username (precisely located using the `p > a` selector).
-   **Queue-Passing Mechanism**: Automatically pushes parsed detail page URLs to the `detail_url_queue` to streamline the crawling task pipeline.

#### 2.1.2 Playlist Detail Crawler Subsystem
-   **Deep Information Extraction**: Obtains rich metadata from playlist detail pages:
    -   Playlist Tag System (multiple tags are joined by "-", defaults to "None" if no tags are present).
    -   Playlist Description (automatically cleans newlines and handles special characters).
    -   Favorite Count (removes formatting characters like parentheses).
    -   Play Count, Song Count, and Comment Count (accurately extracted using distinct CSS selectors).
-   **Exception Handling**: Implements null value detection and default value filling (defaulting to "None") for all data fields to ensure data integrity.

### 2.2 Data Storage and Management Module

#### 2.2.1 Data Storage
-   **Data Storage**: Data is stored in two core CSV files (generated by crawler module 2.1):
    -   `src/music_data/music_list.csv`: Stores basic playlist information (URL, title, play count, creator).
    -   `src/music_data/music_detail.csv`: Stores detailed playlist information (title, tags, description, favorite count, play count, song count, comment count).
-   **Data Source**: The above data files are automatically collected and generated by the crawler subsystem (Section 2.1). Developers must first run the crawler command (execute `scrapy crawl music_list_spider` in the `src/robot_work` directory) to generate the data files before proceeding with subsequent data processing and analysis functionality development.

#### 2.2.3 CSV File Export Layer
-   **Field Mapping**: Automatically handles Chinese character encoding (UTF-8 with BOM) to ensure correct display in tools like Excel.
-   **File Management**: All files are saved uniformly to the `src/music_data` directory, including `music_list.csv` and `music_detail.csv`.

### 2.3 Data Visualization and Analysis Module

#### 2.3.1 Multi-dimensional Statistical Analysis
-   **Song Popularity Analysis**:
    -   A horizontal bar chart of the Top 10 most frequent songs, generated by grouping statistics from the `title` field.
    -   Data Source: Reads `src/music_data/music_detail.csv` file (must be generated by crawler module 2.1 first).
    -   Supports Chinese font rendering (Microsoft YaHei), with a chart size of 16x8 inches and 80 DPI.
    -   Uses a custom color scheme (RGB: 16, 152, 168) with 0.7 transparency to enhance visual appeal.
    -   Output File: `src/music_image/top_10_song.png`
-   **User Contribution Analysis**:
    -   Top 10 creators by playlist contribution, ranked by the number of playlists published.
    -   Data Source: Reads `src/music_data/music_list.csv` file (must be generated by crawler module 2.1 first).
    -   Output File: `src/music_image/top_10_song_up.png`
    -   Top 10 Western-style playlists by play count.
    -   Data Source: Reads `src/music_data/music_detail.csv` file (must be generated by crawler module 2.1 first).
    -   Output File: `src/music_image/top_10_ea_song_playlists.png`
-   **Data Distribution Analysis**:
    -   A histogram analyzing the statistical distribution of play counts.
    -   Data Source: Reads `src/music_data/music_detail.csv` file (must be generated by crawler module 2.1 first).
    -   Output File: `src/music_image/top_10_ea_song_playlists_distribution.png`
-   **Comment Count Analysis**:
    -   Top 10 Western-style playlists by comment count.
    -   Data Source: Reads `src/music_data/music_detail.csv` file (must be generated by crawler module 2.1 first).
    -   Output File: `src/music_image/top_10_of_ea_song_comment.png`

#### 2.3.2 Chart Generation and Export
-   **Progress Bar Display**: A 60-step progress bar (using ▓ and - characters) shows real-time chart generation progress.
-   **Image Saving**: All charts are automatically saved to the `src/music_image` directory in PNG format, supporting high-resolution output.
-   **Command-Line Friendly**: The `plt.show()` interactive display function is removed; only file saving is supported to suit a local command-line environment.
-   **Generation Confirmation**: A confirmation message with the save path is output to the console after each chart is generated.

### 2.4 Data Preprocessing and Cleaning Module

#### 2.4.1 Data Standardization
-   **Value Conversion Engine**:
    -   Play Count Standardization: Converts strings with the Chinese character for "ten thousand" unit to numeric values (e.g., text containing "5" followed by the Chinese "ten thousand" character becomes 50000).
    -   Favorite Count Handling: Replaces the Chinese text for "favorite" with 0 to ensure consistency in numeric fields.
    -   Comment Count Cleaning: Removes formatting characters and converts to integer values.
-   **Data Quality Control**:
    -   Missing Value Handling: `dropna()` automatically deletes records containing null values.
    -   Duplicate Removal: `drop_duplicates()` ensures data uniqueness.
    -   Data Type Validation: Forcibly converts columns to `int` type to ensure accurate numerical calculations.

#### 2.4.2 Cleaned Data Export
-   **Input Files**: Reads `src/music_data/music_list.csv` and `src/music_data/music_detail.csv` (must be generated by crawler module 2.1 first).
-   **Standardized Output**: Generates `src/music_data/cleaned_music_list.csv` and `src/music_data/cleaned_music_detail.csv`.
-   **Field Mapping**: Adds standard column names (`playlist_id`, `title`, `plays`, `creator`, etc.) to the raw data.
-   **Encoding Handling**: Supports UTF-8 encoding to ensure correct display of Chinese characters.
-   **Execution Confirmation**: After data cleaning is completed, outputs to console: "Data cleaning completed, results saved to 'cleaned_music_list.csv' and 'cleaned_music_detail.csv'"

### 2.5 Machine Learning Prediction Module

#### 2.5.1 Play Count Prediction Algorithm
-   **Feature Engineering**: Uses song count (`songs`) and comment count (`comments`) as input features.
-   **Model Architecture**: A regression model trained using the scikit-learn framework (stored as `src/models/play_count_prediction_model_detail.pkl`).
-   **Prediction Service**: Supports batch prediction, taking song count and comment count as input to output predicted play counts.
-   **Result Handling**: Takes the absolute value of negative prediction results to ensure play counts are reasonable.
-   **Test Data**: Includes 20 sets of test data (song count and comment count), with output format: "Sample data X predicted play count: XXXXX"

#### 2.5.2 Model Training Function
-   **Input Data**: Reads `src/music_data/cleaned_music_detail.csv` file (must be generated by data cleaning module 2.4 first).
-   **Model Training**: Uses LinearRegression for regression model training.
-   **Evaluation Metrics**: Outputs Mean Squared Error and R-squared Score evaluation metrics.
-   **Model Saving**: After training is completed, saves the model to `src/models/play_count_prediction_model_detail.pkl`.
-   **Model File Requirements**: The `src/models/play_count_prediction_model_detail.pkl` file must be generated by executing the model training script, or a pre-trained model file can be provided by developers.

#### 2.5.3 Model Management and Deployment
-   **Model Persistence**: Uses `joblib` for model serialization to support fast loading and deployment.
-   **Extensibility Design**: Supports the addition of new features and model retraining to adapt to changing business needs.

### 2.6 Popularity Score Calculation Module

#### 2.6.1 Popularity Score Algorithm
-   **Input Data**: Reads `src/music_data/cleaned_music_detail.csv` file (must be generated by data cleaning module 2.4 first).
-   **Score Formula**: Calculates a comprehensive popularity score based on play count (weight 0.5), song count (weight 0.3), and comment count (weight 0.2).
-   **Output File**: Generates `src/music_data/popularity_ranking_detail.csv`, sorted by popularity score in descending order.
-   **Execution Confirmation**: After calculation is completed, outputs to console: "Popularity score calculation completed, results saved to 'popularity_ranking_detail.csv'"

### 2.7 Clustering Analysis Module

#### 2.7.1 Playlist Clustering Analysis
-   **Input Data**: Reads `src/music_data/cleaned_music_detail.csv` file (must be generated by data cleaning module 2.4 first).
-   **Feature Selection**: Uses play count (`play`), song count (`songs`), and comment count (`comments`) as clustering features.
-   **Clustering Algorithm**: Uses KMeans clustering algorithm, determining the optimal number of clusters through the Elbow Method (maximum 3 clusters).
-   **Exception Handling**: When the number of data samples is less than 3, all data is assigned to the same cluster (Cluster=0), and outputs corresponding prompt information.
-   **Output File**: Generates `src/music_data/clustered_music_detail.csv`, containing cluster labels.
-   **Execution Confirmation**: After clustering is completed, outputs corresponding completion information ("Clustering model training completed, results saved to 'clustered_music_detail.csv'" or "Due to insufficient samples, all data assigned to the same cluster, results saved to 'clustered_music_detail.csv'")

### 2.8 User Preference Analysis Module

#### 2.8.1 Data Distribution Visualization
-   **Input Data**: Reads `src/music_data/cleaned_music_detail.csv` file (must be generated by data cleaning module 2.4 first).
-   **Analysis Content**: Draws distribution histograms (with KDE curves) for play count, song count, and comment count.
-   **Chart Style**: Uses different colors to distinguish (blue for play count, green for song count, red for comment count).
-   **Output File**: Generates `src/music_image/user_preference_analysis.png`.
-   **Execution Confirmation**: After analysis is completed, outputs to console: "User preference analysis chart saved to src/music_image/user_preference_analysis.png"

### 2.9 System Interaction and Control Module

#### 2.9.1 Command-Line Interface (CLI)
-   **Menu System**: When the program starts, clears the screen and displays the main menu containing the title "【NetEase Cloud Music Data Analysis System】".
-   **Function Navigation**: Provides 5 data visualization options (A-E) for user selection:
    -   A: Generate Top 10 Song Occurrence Chart.
    -   B: Generate Top 10 Playlist Contribution by Creator Chart.
    -   C: Generate Top 10 Western Playlists by Play Count Chart.
    -   D: Generate Top 10 Western Playlists by Comment Count Chart.
    -   E: Generate Play Count Distribution Chart.
-   **Exit Mechanism**: Supports input of 'quit' or 'QUIT' command for safe system exit, outputs "Exited!" when exiting.

#### 2.9.2 Batch Execution Mode
-   **Automated Execution**: `main.py` supports a non-interactive mode that automatically executes all visualization tasks.
-   **Screen Clearing**: Uses `os.system('cls')` to clear the screen, enhancing the user experience.
-   **Exception Handling**: Includes a complete error-capturing and logging mechanism.

## Technical Requirements

### 3.1 System Architecture Requirements

#### 3.1.1 Distributed Crawler Architecture
-   **Core Framework**: Based on Scrapy 2.11.2.
-   **Fault Tolerance**: Implements checkpoint-resume, retry-on-failure, and exception recovery to ensure data collection stability.

### 3.2 Tech Stack and Dependency Management

#### 3.2.1 Python Environment Requirements
-   **Python Version**: Python 3.8+, compatible with asynchronous programming and type hints.
-   **Core Dependencies**:
    -   Crawler Engine: `Scrapy==2.11.2`
    -   Data Processing: `pandas==2.2.3`, `numpy==2.1.3`
    -   Data Visualization: `matplotlib` (configured with a non-GUI backend for file saving only).
    -   Machine Learning: `scikit-learn`, `joblib` (for model persistence).

#### 3.2.2 Third-Party Service Integration
-   **HTTP Client**: `requests==2.32.3` + `pyppeteer==2.0.0` for dynamic page rendering.
-   **Data Parsing**: `beautifulsoup4==4.12.3`, `lxml==5.3.0`, `cssselect==1.2.0`.
-   **Security**: `cryptography==43.0.3`, `pyOpenSSL==24.2.1` to ensure secure data transmission.

### 3.3 Deployment Environment Requirements

#### 3.3.1 Local Environment Configuration
-   **Operating System**: Windows 10+, macOS 10.15+, or a Linux distribution that supports command-line operations.
-   **Hardware Requirements**:
    -   CPU: Dual-core or higher.
    -   Memory: 4GB or more to meet data processing demands.
    -   Storage: 20GB or more of available space.
-   **Network Environment**: A stable internet connection with support for HTTP/HTTPS protocols.

#### 3.3.2 Local Service Installation
-   **Python Environment**: Use `virtualenv` or `conda` to create an isolated environment to avoid package conflicts.
-   **Matplotlib Backend**: Configure the `Agg` backend (non-GUI) to support chart generation in a local environment.

### 3.4 Performance and Monitoring Requirements

#### 3.4.1 Performance Metrics
-   **Crawling Efficiency**: Support for processing 20+ URL requests per second, with a daily collection volume of over 50,000 playlists in a local environment.
-   **Data Processing**: `pandas` data processing should support hundreds of thousands of records, with processing times kept to within minutes.
-   **Visualization Response**: Chart generation time should be under 30 seconds, with a real-time progress bar display.
-   **Storage Performance**: Local MongoDB write performance must meet data storage needs; Redis queue operations must be smooth.

### 3.5 Security and Compliance Requirements

#### 3.5.1 Compliance Requirements
-   **Crawler Etiquette**: Adhere to `robots.txt` protocols; set reasonable request intervals and concurrency limits.
-   **Data Usage**: Follow NetEase Cloud Music's terms of service; data is to be used for academic and non-commercial purposes only.
-   **Privacy Protection**: Do not collect users' personal privacy information; comply with data protection regulations such as GDPR.
-   **Open Source License**: Code adheres to the MIT open-source license; dependency licenses must be compatible.

### 3.6 Operations and Deployment Requirements

#### 3.6.1 Local Installation Process
-   **Dependency Installation**: The `src` directory must contain a `requirements.txt` file, which is used for batch installation and version locking of Python packages.
-   **Configuration Management**: Manage database connection strings, Redis configurations, etc., through configuration files.
-   **Startup Scripts**: Provide simple startup methods that support running the crawler, data processing, and analysis tasks.
-   **Crawler Execution**: Execute the crawler command `scrapy crawl music_list_spider` in the `src/robot_work` directory to start the crawler.

#### 3.6.2 Maintenance and Extensibility
-   **Code Structure**: A modular design that supports independent maintenance of the crawler, data processing, and analysis/prediction modules.
-   **Error Handling**: A comprehensive exception-capturing and error-recovery mechanism to prevent single points of failure from crashing the system.
-   **Extension Interfaces**: Reserve extension points for adding new data sources, new analysis dimensions, and model algorithm upgrades.
-   **Documentation**: Maintain a complete README, API documentation, and deployment guide to facilitate team collaboration and knowledge transfer.

### 3.7 Command-Line Interface Optimization Requirements

#### 3.7.1 User Experience Design
-   **Progress Visualization**: Provide progress bars for long-running tasks (e.g., data processing, chart generation).
-   **Operation Confirmation**: Provide confirmation prompts for critical operations (e.g., data cleaning, file overwrites) to prevent mistakes.
-   **Result Feedback**: Provide clear success/failure feedback and the result file path after each operation.
-   **User-Friendly Errors**: Provide clear error messages and suggested solutions in exceptional cases.